
==> Audit <==
|---------|--------------|----------|-------|---------|---------------------|---------------------|
| Command |     Args     | Profile  | User  | Version |     Start Time      |      End Time       |
|---------|--------------|----------|-------|---------|---------------------|---------------------|
| start   |              | minikube | didin | v1.36.0 | 18 Jun 25 10:14 WIB | 18 Jun 25 10:16 WIB |
| service | quarkus-demo | minikube | didin | v1.36.0 | 18 Jun 25 10:18 WIB |                     |
|---------|--------------|----------|-------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/06/18 10:14:29
Running on machine: Didins-Mac-mini
Binary: Built with gc go1.24.0 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0618 10:14:29.633379   17469 out.go:345] Setting OutFile to fd 1 ...
I0618 10:14:29.633564   17469 out.go:397] isatty.IsTerminal(1) = true
I0618 10:14:29.633566   17469 out.go:358] Setting ErrFile to fd 2...
I0618 10:14:29.633569   17469 out.go:397] isatty.IsTerminal(2) = true
I0618 10:14:29.633698   17469 root.go:338] Updating PATH: /Users/didin/.minikube/bin
W0618 10:14:29.633811   17469 root.go:314] Error reading config file at /Users/didin/.minikube/config/config.json: open /Users/didin/.minikube/config/config.json: no such file or directory
I0618 10:14:29.634331   17469 out.go:352] Setting JSON to false
I0618 10:14:29.655030   17469 start.go:130] hostinfo: {"hostname":"Didins-Mac-mini.local","uptime":358626,"bootTime":1749857843,"procs":403,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"15.5","kernelVersion":"24.5.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"08c7e17b-d0d0-59fb-97ff-2c5cfd7e504c"}
W0618 10:14:29.655080   17469 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0618 10:14:29.659216   17469 out.go:177] 😄  minikube v1.36.0 on Darwin 15.5 (arm64)
I0618 10:14:29.666265   17469 notify.go:220] Checking for updates...
W0618 10:14:29.666286   17469 preload.go:293] Failed to list preload files: open /Users/didin/.minikube/cache/preloaded-tarball: no such file or directory
I0618 10:14:29.666414   17469 driver.go:404] Setting default libvirt URI to qemu:///system
I0618 10:14:29.666450   17469 global.go:112] Querying for installed drivers using PATH=/Users/didin/.minikube/bin:/Users/didin/development/flutter/bin:/Users/didin/Library/Android/sdk/emulator:/Users/didin/Library/Android/sdk/tools:/Users/didin/Library/Android/sdk/tools/bin:/Users/didin/Library/Android/sdk/platform-tools:/Users/didin/.jenv/shims:/Users/didin/.jenv/bin:/opt/homebrew/opt/mongodb-community@4.4/bin:/Users/didin/.sdkman/candidates/quarkus/current/bin:/Users/didin/.sdkman/candidates/maven/current/bin:/Users/didin/.sdkman/candidates/java/current/bin:/Users/didin/.sdkman/candidates/grails/current/bin:/Users/didin/.sdkman/candidates/gradle/current/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/Applications/Wireshark.app/Contents/MacOS:/usr/local/go/bin:/Users/didin/.cargo/bin:/Users/didin/development/flutter/bin:/Users/didin/.sdkman/candidates/java/17.0.12-oracle/bin:/Users/didin/Library/Android/sdk/platform-tools
I0618 10:14:29.706315   17469 docker.go:123] docker version: linux-28.1.1:Docker Desktop 4.41.2 (191736)
I0618 10:14:29.706499   17469 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0618 10:14:30.095639   17469 info.go:266] docker info: {ID:600f1161-0311-4eb3-a3a5-9a2e99e9ca20 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:6 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:67 OomKillDisable:false NGoroutines:89 SystemTime:2025-06-18 03:14:30.079162252 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4109803520 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/didin/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/didin/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:/Users/didin/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:/Users/didin/.docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:/Users/didin/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:/Users/didin/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/Users/didin/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:/Users/didin/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/didin/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:/Users/didin/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/Users/didin/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:/Users/didin/.docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.11] map[Name:sbom Path:/Users/didin/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/didin/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0618 10:14:30.095724   17469 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
W0618 10:14:31.191402   17469 podman.go:138] podman returned error: exit status 125
I0618 10:14:31.191600   17469 global.go:133] podman default: true priority: 3, state: {Installed:true Healthy:false Running:false NeedsImprovement:false Error:"podman version --format {{.Server.Version}}" exit status 125: Cannot connect to Podman. Please verify your connection to the Linux system using `podman system connection list`, or try `podman machine init` and `podman machine start` to manage a new Linux VM
Error: unable to connect to Podman socket: Get "http://d/v5.1.2/libpod/_ping": dial unix /var/folders/pn/ff2s79ps7c75l2cntpz_y0dc0000gn/T/storage-run-501/podman/podman.sock: connect: no such file or directory Reason: Fix: Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0618 10:14:31.191716   17469 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0618 10:14:31.192279   17469 global.go:133] hyperkit default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "hyperkit": executable file not found in $PATH Reason: Fix:Run 'brew install hyperkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/hyperkit/ Version:}
I0618 10:14:31.192497   17469 global.go:133] parallels default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "prlctl": executable file not found in $PATH Reason: Fix:Install Parallels Desktop for Mac Doc:https://minikube.sigs.k8s.io/docs/drivers/parallels/ Version:}
I0618 10:14:31.195288   17469 global.go:133] qemu2 default: true priority: 7, state: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0618 10:14:31.195736   17469 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0618 10:14:31.196084   17469 global.go:133] vfkit default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vfkit": executable file not found in $PATH Reason: Fix:Run 'brew install vfkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vfkit/ Version:}
I0618 10:14:31.196274   17469 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0618 10:14:31.196337   17469 driver.go:326] not recommending "ssh" due to default: false
I0618 10:14:31.196344   17469 driver.go:321] not recommending "podman" due to health: "podman version --format {{.Server.Version}}" exit status 125: Cannot connect to Podman. Please verify your connection to the Linux system using `podman system connection list`, or try `podman machine init` and `podman machine start` to manage a new Linux VM
Error: unable to connect to Podman socket: Get "http://d/v5.1.2/libpod/_ping": dial unix /var/folders/pn/ff2s79ps7c75l2cntpz_y0dc0000gn/T/storage-run-501/podman/podman.sock: connect: no such file or directory
I0618 10:14:31.196368   17469 driver.go:361] Picked: docker
I0618 10:14:31.196386   17469 driver.go:362] Alternatives: [qemu2 ssh]
I0618 10:14:31.196394   17469 driver.go:363] Rejects: [podman hyperkit parallels virtualbox vfkit vmware]
I0618 10:14:31.202783   17469 out.go:177] ✨  Automatically selected the docker driver. Other choices: qemu2, ssh
I0618 10:14:31.205865   17469 start.go:304] selected driver: docker
I0618 10:14:31.205895   17469 start.go:908] validating driver "docker" against <nil>
I0618 10:14:31.205924   17469 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0618 10:14:31.206397   17469 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0618 10:14:31.317157   17469 info.go:266] docker info: {ID:600f1161-0311-4eb3-a3a5-9a2e99e9ca20 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:6 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:67 OomKillDisable:false NGoroutines:89 SystemTime:2025-06-18 03:14:31.306339461 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4109803520 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/didin/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/didin/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:/Users/didin/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:/Users/didin/.docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:/Users/didin/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:/Users/didin/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/Users/didin/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:/Users/didin/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/didin/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:/Users/didin/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/Users/didin/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:/Users/didin/.docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.11] map[Name:sbom Path:/Users/didin/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/didin/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0618 10:14:31.317277   17469 start_flags.go:311] no existing cluster config was found, will generate one from the flags 
I0618 10:14:31.320944   17469 start_flags.go:394] Using suggested 2200MB memory alloc based on sys=8192MB, container=3919MB
I0618 10:14:31.321587   17469 start_flags.go:958] Wait components to verify : map[apiserver:true system_pods:true]
I0618 10:14:31.324090   17469 out.go:177] 📌  Using Docker Desktop driver with root privileges
I0618 10:14:31.325842   17469 cni.go:84] Creating CNI manager for ""
I0618 10:14:31.325896   17469 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0618 10:14:31.325905   17469 start_flags.go:320] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0618 10:14:31.325964   17469 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0618 10:14:31.328758   17469 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0618 10:14:31.333839   17469 cache.go:121] Beginning downloading kic base image for docker with docker
I0618 10:14:31.336737   17469 out.go:177] 🚜  Pulling base image v0.0.47 ...
I0618 10:14:31.340773   17469 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0618 10:14:31.340797   17469 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0618 10:14:31.420781   17469 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b to local cache
I0618 10:14:31.421035   17469 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory
I0618 10:14:31.421146   17469 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b to local cache
I0618 10:14:31.539072   17469 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.33.1/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-arm64.tar.lz4
I0618 10:14:31.539089   17469 cache.go:56] Caching tarball of preloaded images
I0618 10:14:31.539411   17469 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0618 10:14:31.547919   17469 out.go:177] 💾  Downloading Kubernetes v1.33.1 preload ...
I0618 10:14:31.550785   17469 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.33.1-docker-overlay2-arm64.tar.lz4 ...
I0618 10:14:32.177510   17469 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.33.1/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-arm64.tar.lz4?checksum=md5:c36f27cf532b9720a074b76fa1b6b6cb -> /Users/didin/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-arm64.tar.lz4
I0618 10:15:23.629437   17469 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.33.1-docker-overlay2-arm64.tar.lz4 ...
I0618 10:15:23.629591   17469 preload.go:254] verifying checksum of /Users/didin/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-arm64.tar.lz4 ...
I0618 10:15:24.301280   17469 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0618 10:15:24.307181   17469 profile.go:143] Saving config to /Users/didin/.minikube/profiles/minikube/config.json ...
I0618 10:15:24.308068   17469 lock.go:35] WriteFile acquiring /Users/didin/.minikube/profiles/minikube/config.json: {Name:mk1a2f6770f5da972e325c79ed64731826179e7a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 10:16:02.744936   17469 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b as a tarball
I0618 10:16:02.745239   17469 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from local cache
I0618 10:16:14.135858   17469 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from cached tarball
I0618 10:16:14.136843   17469 cache.go:230] Successfully downloaded all kic artifacts
I0618 10:16:14.139796   17469 start.go:360] acquireMachinesLock for minikube: {Name:mk354c977b0119a1d2ca735a8e56aa113a3081ee Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0618 10:16:14.141536   17469 start.go:364] duration metric: took 1.447833ms to acquireMachinesLock for "minikube"
I0618 10:16:14.142276   17469 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0618 10:16:14.143870   17469 start.go:125] createHost starting for "" (driver="docker")
I0618 10:16:14.153665   17469 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=2200MB) ...
I0618 10:16:14.156909   17469 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0618 10:16:14.157091   17469 client.go:168] LocalClient.Create starting
I0618 10:16:14.162985   17469 main.go:141] libmachine: Creating CA: /Users/didin/.minikube/certs/ca.pem
I0618 10:16:14.406272   17469 main.go:141] libmachine: Creating client certificate: /Users/didin/.minikube/certs/cert.pem
I0618 10:16:14.467781   17469 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0618 10:16:14.484078   17469 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0618 10:16:14.484370   17469 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0618 10:16:14.484378   17469 cli_runner.go:164] Run: docker network inspect minikube
W0618 10:16:14.500921   17469 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0618 10:16:14.500938   17469 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0618 10:16:14.500960   17469 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0618 10:16:14.501250   17469 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0618 10:16:14.516778   17469 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0x14000d7d250}
I0618 10:16:14.516948   17469 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 65535 ...
I0618 10:16:14.517203   17469 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0618 10:16:14.568300   17469 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0618 10:16:14.568964   17469 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0618 10:16:14.570227   17469 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0618 10:16:14.586872   17469 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0618 10:16:14.620781   17469 oci.go:103] Successfully created a docker volume minikube
I0618 10:16:14.621107   17469 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib
I0618 10:16:15.314109   17469 oci.go:107] Successfully prepared a docker volume minikube
I0618 10:16:15.318310   17469 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0618 10:16:15.320796   17469 kic.go:194] Starting extracting preloaded images to volume ...
I0618 10:16:15.322056   17469 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/didin/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir
I0618 10:16:21.071010   17469 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/didin/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir: (5.748923834s)
I0618 10:16:21.071097   17469 kic.go:203] duration metric: took 5.751041916s to extract preloaded images to volume ...
I0618 10:16:21.072869   17469 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0618 10:16:22.137771   17469 cli_runner.go:217] Completed: docker info --format "'{{json .SecurityOptions}}'": (1.064868375s)
I0618 10:16:22.137988   17469 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b
I0618 10:16:22.347787   17469 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0618 10:16:22.371396   17469 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0618 10:16:22.390232   17469 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0618 10:16:22.448563   17469 oci.go:144] the created container "minikube" has a running status.
I0618 10:16:22.448590   17469 kic.go:225] Creating ssh key for kic: /Users/didin/.minikube/machines/minikube/id_rsa...
I0618 10:16:22.645410   17469 kic_runner.go:191] docker (temp): /Users/didin/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0618 10:16:22.711849   17469 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0618 10:16:22.732893   17469 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0618 10:16:22.732910   17469 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0618 10:16:22.769002   17469 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0618 10:16:22.785623   17469 machine.go:93] provisionDockerMachine start ...
I0618 10:16:22.787407   17469 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 10:16:22.801889   17469 main.go:141] libmachine: Using SSH client type: native
I0618 10:16:22.802198   17469 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102fe9110] 0x102feb8d0 <nil>  [] 0s} 127.0.0.1 54074 <nil> <nil>}
I0618 10:16:22.802202   17469 main.go:141] libmachine: About to run SSH command:
hostname
I0618 10:16:22.919544   17469 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0618 10:16:22.921128   17469 ubuntu.go:169] provisioning hostname "minikube"
I0618 10:16:22.923417   17469 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 10:16:22.941299   17469 main.go:141] libmachine: Using SSH client type: native
I0618 10:16:22.941498   17469 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102fe9110] 0x102feb8d0 <nil>  [] 0s} 127.0.0.1 54074 <nil> <nil>}
I0618 10:16:22.941507   17469 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0618 10:16:23.052156   17469 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0618 10:16:23.052512   17469 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 10:16:23.072859   17469 main.go:141] libmachine: Using SSH client type: native
I0618 10:16:23.073081   17469 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102fe9110] 0x102feb8d0 <nil>  [] 0s} 127.0.0.1 54074 <nil> <nil>}
I0618 10:16:23.073089   17469 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0618 10:16:23.182407   17469 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0618 10:16:23.182448   17469 ubuntu.go:175] set auth options {CertDir:/Users/didin/.minikube CaCertPath:/Users/didin/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/didin/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/didin/.minikube/machines/server.pem ServerKeyPath:/Users/didin/.minikube/machines/server-key.pem ClientKeyPath:/Users/didin/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/didin/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/didin/.minikube}
I0618 10:16:23.182602   17469 ubuntu.go:177] setting up certificates
I0618 10:16:23.182825   17469 provision.go:84] configureAuth start
I0618 10:16:23.183234   17469 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0618 10:16:23.214546   17469 provision.go:143] copyHostCerts
I0618 10:16:23.214826   17469 exec_runner.go:151] cp: /Users/didin/.minikube/certs/ca.pem --> /Users/didin/.minikube/ca.pem (1074 bytes)
I0618 10:16:23.215128   17469 exec_runner.go:151] cp: /Users/didin/.minikube/certs/cert.pem --> /Users/didin/.minikube/cert.pem (1119 bytes)
I0618 10:16:23.215371   17469 exec_runner.go:151] cp: /Users/didin/.minikube/certs/key.pem --> /Users/didin/.minikube/key.pem (1675 bytes)
I0618 10:16:23.216287   17469 provision.go:117] generating server cert: /Users/didin/.minikube/machines/server.pem ca-key=/Users/didin/.minikube/certs/ca.pem private-key=/Users/didin/.minikube/certs/ca-key.pem org=didin.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0618 10:16:23.361276   17469 provision.go:177] copyRemoteCerts
I0618 10:16:23.361583   17469 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0618 10:16:23.361641   17469 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 10:16:23.380469   17469 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54074 SSHKeyPath:/Users/didin/.minikube/machines/minikube/id_rsa Username:docker}
I0618 10:16:23.467645   17469 ssh_runner.go:362] scp /Users/didin/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0618 10:16:23.486743   17469 ssh_runner.go:362] scp /Users/didin/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0618 10:16:23.500671   17469 ssh_runner.go:362] scp /Users/didin/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0618 10:16:23.512466   17469 provision.go:87] duration metric: took 329.421083ms to configureAuth
I0618 10:16:23.512474   17469 ubuntu.go:193] setting minikube options for container-runtime
I0618 10:16:23.513212   17469 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0618 10:16:23.513492   17469 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 10:16:23.533837   17469 main.go:141] libmachine: Using SSH client type: native
I0618 10:16:23.534024   17469 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102fe9110] 0x102feb8d0 <nil>  [] 0s} 127.0.0.1 54074 <nil> <nil>}
I0618 10:16:23.534032   17469 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0618 10:16:23.639278   17469 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0618 10:16:23.639296   17469 ubuntu.go:71] root file system type: overlay
I0618 10:16:23.641017   17469 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0618 10:16:23.641497   17469 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 10:16:23.674206   17469 main.go:141] libmachine: Using SSH client type: native
I0618 10:16:23.674515   17469 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102fe9110] 0x102feb8d0 <nil>  [] 0s} 127.0.0.1 54074 <nil> <nil>}
I0618 10:16:23.674582   17469 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0618 10:16:23.800239   17469 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0618 10:16:23.800818   17469 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 10:16:23.835242   17469 main.go:141] libmachine: Using SSH client type: native
I0618 10:16:23.835531   17469 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x102fe9110] 0x102feb8d0 <nil>  [] 0s} 127.0.0.1 54074 <nil> <nil>}
I0618 10:16:23.835543   17469 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0618 10:16:24.998544   17469 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-04-18 09:50:43.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-06-18 03:16:23.796359013 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0618 10:16:24.998568   17469 machine.go:96] duration metric: took 2.212974875s to provisionDockerMachine
I0618 10:16:24.998586   17469 client.go:171] duration metric: took 10.84169975s to LocalClient.Create
I0618 10:16:24.998616   17469 start.go:167] duration metric: took 10.841926417s to libmachine.API.Create "minikube"
I0618 10:16:24.998622   17469 start.go:293] postStartSetup for "minikube" (driver="docker")
I0618 10:16:24.998780   17469 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0618 10:16:24.998963   17469 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0618 10:16:24.999075   17469 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 10:16:25.029665   17469 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54074 SSHKeyPath:/Users/didin/.minikube/machines/minikube/id_rsa Username:docker}
I0618 10:16:25.118637   17469 ssh_runner.go:195] Run: cat /etc/os-release
I0618 10:16:25.123438   17469 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0618 10:16:25.123524   17469 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0618 10:16:25.123545   17469 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0618 10:16:25.123555   17469 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0618 10:16:25.123569   17469 filesync.go:126] Scanning /Users/didin/.minikube/addons for local assets ...
I0618 10:16:25.124171   17469 filesync.go:126] Scanning /Users/didin/.minikube/files for local assets ...
I0618 10:16:25.124310   17469 start.go:296] duration metric: took 125.681959ms for postStartSetup
I0618 10:16:25.126142   17469 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0618 10:16:25.160253   17469 profile.go:143] Saving config to /Users/didin/.minikube/profiles/minikube/config.json ...
I0618 10:16:25.161021   17469 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0618 10:16:25.161320   17469 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 10:16:25.187946   17469 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54074 SSHKeyPath:/Users/didin/.minikube/machines/minikube/id_rsa Username:docker}
I0618 10:16:25.271805   17469 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0618 10:16:25.276147   17469 start.go:128] duration metric: took 11.132466209s to createHost
I0618 10:16:25.276404   17469 start.go:83] releasing machines lock for "minikube", held for 11.134838167s
I0618 10:16:25.277455   17469 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0618 10:16:25.319756   17469 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0618 10:16:25.320116   17469 ssh_runner.go:195] Run: cat /version.json
I0618 10:16:25.320248   17469 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 10:16:25.321787   17469 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 10:16:25.346703   17469 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54074 SSHKeyPath:/Users/didin/.minikube/machines/minikube/id_rsa Username:docker}
I0618 10:16:25.346697   17469 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54074 SSHKeyPath:/Users/didin/.minikube/machines/minikube/id_rsa Username:docker}
I0618 10:16:25.803105   17469 ssh_runner.go:195] Run: systemctl --version
I0618 10:16:25.808883   17469 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0618 10:16:25.812871   17469 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0618 10:16:25.832672   17469 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0618 10:16:25.833142   17469 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0618 10:16:25.846749   17469 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0618 10:16:25.847118   17469 start.go:495] detecting cgroup driver to use...
I0618 10:16:25.847355   17469 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0618 10:16:25.848544   17469 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0618 10:16:25.856915   17469 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0618 10:16:25.862156   17469 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0618 10:16:25.866476   17469 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0618 10:16:25.866706   17469 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0618 10:16:25.871027   17469 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0618 10:16:25.874933   17469 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0618 10:16:25.878840   17469 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0618 10:16:25.882758   17469 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0618 10:16:25.886637   17469 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0618 10:16:25.890408   17469 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0618 10:16:25.894265   17469 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0618 10:16:25.898204   17469 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0618 10:16:25.901554   17469 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0618 10:16:25.905018   17469 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0618 10:16:25.934731   17469 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0618 10:16:25.985292   17469 start.go:495] detecting cgroup driver to use...
I0618 10:16:25.985313   17469 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0618 10:16:25.985779   17469 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0618 10:16:25.991006   17469 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0618 10:16:25.991346   17469 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0618 10:16:25.996355   17469 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0618 10:16:26.002779   17469 ssh_runner.go:195] Run: which cri-dockerd
I0618 10:16:26.004494   17469 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0618 10:16:26.009351   17469 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0618 10:16:26.016612   17469 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0618 10:16:26.045706   17469 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0618 10:16:26.070434   17469 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0618 10:16:26.075264   17469 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0618 10:16:26.082161   17469 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0618 10:16:26.087723   17469 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0618 10:16:26.112144   17469 ssh_runner.go:195] Run: sudo systemctl restart docker
I0618 10:16:27.618518   17469 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.506377833s)
I0618 10:16:27.618726   17469 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0618 10:16:27.624988   17469 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0618 10:16:27.630888   17469 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0618 10:16:27.665061   17469 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0618 10:16:27.700332   17469 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0618 10:16:27.731946   17469 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0618 10:16:27.748720   17469 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0618 10:16:27.753339   17469 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0618 10:16:27.778660   17469 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0618 10:16:27.839995   17469 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0618 10:16:27.845075   17469 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0618 10:16:27.850900   17469 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0618 10:16:27.852639   17469 start.go:563] Will wait 60s for crictl version
I0618 10:16:27.852873   17469 ssh_runner.go:195] Run: which crictl
I0618 10:16:27.854971   17469 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0618 10:16:27.883493   17469 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0618 10:16:27.883829   17469 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0618 10:16:27.911633   17469 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0618 10:16:27.991784   17469 out.go:235] 🐳  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0618 10:16:27.999687   17469 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0618 10:16:28.132072   17469 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0618 10:16:28.132916   17469 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0618 10:16:28.136814   17469 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0618 10:16:28.146524   17469 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0618 10:16:28.178856   17469 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0618 10:16:28.181874   17469 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0618 10:16:28.181985   17469 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0618 10:16:28.194291   17469 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0618 10:16:28.194300   17469 docker.go:632] Images already preloaded, skipping extraction
I0618 10:16:28.196792   17469 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0618 10:16:28.206443   17469 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0618 10:16:28.206842   17469 cache_images.go:84] Images are preloaded, skipping loading
I0618 10:16:28.206851   17469 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0618 10:16:28.207676   17469 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0618 10:16:28.207794   17469 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0618 10:16:28.249642   17469 cni.go:84] Creating CNI manager for ""
I0618 10:16:28.249662   17469 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0618 10:16:28.249842   17469 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0618 10:16:28.249909   17469 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0618 10:16:28.250491   17469 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0618 10:16:28.250723   17469 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0618 10:16:28.255936   17469 binaries.go:44] Found k8s binaries, skipping transfer
I0618 10:16:28.256105   17469 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0618 10:16:28.261074   17469 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0618 10:16:28.268785   17469 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0618 10:16:28.276485   17469 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0618 10:16:28.284125   17469 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0618 10:16:28.285919   17469 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0618 10:16:28.290340   17469 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0618 10:16:28.320423   17469 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0618 10:16:28.339097   17469 certs.go:68] Setting up /Users/didin/.minikube/profiles/minikube for IP: 192.168.49.2
I0618 10:16:28.339123   17469 certs.go:194] generating shared ca certs ...
I0618 10:16:28.339139   17469 certs.go:226] acquiring lock for ca certs: {Name:mk6ed2683efc396f92bda4782a7ef06300d45858 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 10:16:28.340225   17469 certs.go:240] generating "minikubeCA" ca cert: /Users/didin/.minikube/ca.key
I0618 10:16:28.486497   17469 crypto.go:156] Writing cert to /Users/didin/.minikube/ca.crt ...
I0618 10:16:28.487992   17469 lock.go:35] WriteFile acquiring /Users/didin/.minikube/ca.crt: {Name:mkae5f0f5f0a72387eaaadeee355a80b81074fc8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 10:16:28.488285   17469 crypto.go:164] Writing key to /Users/didin/.minikube/ca.key ...
I0618 10:16:28.488288   17469 lock.go:35] WriteFile acquiring /Users/didin/.minikube/ca.key: {Name:mk81ab877f3963190d298b492bd8933d21ea0eb1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 10:16:28.488427   17469 certs.go:240] generating "proxyClientCA" ca cert: /Users/didin/.minikube/proxy-client-ca.key
I0618 10:16:28.540859   17469 crypto.go:156] Writing cert to /Users/didin/.minikube/proxy-client-ca.crt ...
I0618 10:16:28.540873   17469 lock.go:35] WriteFile acquiring /Users/didin/.minikube/proxy-client-ca.crt: {Name:mka06621a6083c579b7515280885a3df754ff37c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 10:16:28.541127   17469 crypto.go:164] Writing key to /Users/didin/.minikube/proxy-client-ca.key ...
I0618 10:16:28.541130   17469 lock.go:35] WriteFile acquiring /Users/didin/.minikube/proxy-client-ca.key: {Name:mk9aa1a46733e6fedbe23c972e40f31fa6f89a5e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 10:16:28.541448   17469 certs.go:256] generating profile certs ...
I0618 10:16:28.541494   17469 certs.go:363] generating signed profile cert for "minikube-user": /Users/didin/.minikube/profiles/minikube/client.key
I0618 10:16:28.541502   17469 crypto.go:68] Generating cert /Users/didin/.minikube/profiles/minikube/client.crt with IP's: []
I0618 10:16:28.563746   17469 crypto.go:156] Writing cert to /Users/didin/.minikube/profiles/minikube/client.crt ...
I0618 10:16:28.563751   17469 lock.go:35] WriteFile acquiring /Users/didin/.minikube/profiles/minikube/client.crt: {Name:mkcdc7fddfb21ddc94fbff970366c5b4d657490e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 10:16:28.563959   17469 crypto.go:164] Writing key to /Users/didin/.minikube/profiles/minikube/client.key ...
I0618 10:16:28.563961   17469 lock.go:35] WriteFile acquiring /Users/didin/.minikube/profiles/minikube/client.key: {Name:mka43337ea48689f40559dc016bb83e3993f74af Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 10:16:28.564134   17469 certs.go:363] generating signed profile cert for "minikube": /Users/didin/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0618 10:16:28.564145   17469 crypto.go:68] Generating cert /Users/didin/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0618 10:16:29.049701   17469 crypto.go:156] Writing cert to /Users/didin/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0618 10:16:29.049721   17469 lock.go:35] WriteFile acquiring /Users/didin/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mkba4a0f4f99c63e064b8cca4bc1ff5af63eb71b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 10:16:29.050015   17469 crypto.go:164] Writing key to /Users/didin/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0618 10:16:29.050019   17469 lock.go:35] WriteFile acquiring /Users/didin/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk5b53dde0719613ae7920e8a3311bd4adc3a330 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 10:16:29.050163   17469 certs.go:381] copying /Users/didin/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /Users/didin/.minikube/profiles/minikube/apiserver.crt
I0618 10:16:29.050768   17469 certs.go:385] copying /Users/didin/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /Users/didin/.minikube/profiles/minikube/apiserver.key
I0618 10:16:29.050849   17469 certs.go:363] generating signed profile cert for "aggregator": /Users/didin/.minikube/profiles/minikube/proxy-client.key
I0618 10:16:29.050859   17469 crypto.go:68] Generating cert /Users/didin/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0618 10:16:29.196658   17469 crypto.go:156] Writing cert to /Users/didin/.minikube/profiles/minikube/proxy-client.crt ...
I0618 10:16:29.196676   17469 lock.go:35] WriteFile acquiring /Users/didin/.minikube/profiles/minikube/proxy-client.crt: {Name:mk1338d4f11934de4700ab55dfab8539a17ef40e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 10:16:29.197188   17469 crypto.go:164] Writing key to /Users/didin/.minikube/profiles/minikube/proxy-client.key ...
I0618 10:16:29.197196   17469 lock.go:35] WriteFile acquiring /Users/didin/.minikube/profiles/minikube/proxy-client.key: {Name:mkfb3daa16b8ac651f63ecbd5f48504221b07fed Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 10:16:29.200408   17469 certs.go:484] found cert: /Users/didin/.minikube/certs/ca-key.pem (1675 bytes)
I0618 10:16:29.202099   17469 certs.go:484] found cert: /Users/didin/.minikube/certs/ca.pem (1074 bytes)
I0618 10:16:29.202127   17469 certs.go:484] found cert: /Users/didin/.minikube/certs/cert.pem (1119 bytes)
I0618 10:16:29.202516   17469 certs.go:484] found cert: /Users/didin/.minikube/certs/key.pem (1675 bytes)
I0618 10:16:29.261584   17469 ssh_runner.go:362] scp /Users/didin/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0618 10:16:29.291636   17469 ssh_runner.go:362] scp /Users/didin/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0618 10:16:29.303654   17469 ssh_runner.go:362] scp /Users/didin/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0618 10:16:29.315202   17469 ssh_runner.go:362] scp /Users/didin/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0618 10:16:29.327274   17469 ssh_runner.go:362] scp /Users/didin/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0618 10:16:29.339417   17469 ssh_runner.go:362] scp /Users/didin/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1671 bytes)
I0618 10:16:29.351284   17469 ssh_runner.go:362] scp /Users/didin/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0618 10:16:29.361097   17469 ssh_runner.go:362] scp /Users/didin/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0618 10:16:29.374470   17469 ssh_runner.go:362] scp /Users/didin/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0618 10:16:29.388745   17469 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0618 10:16:29.404232   17469 ssh_runner.go:195] Run: openssl version
I0618 10:16:29.408438   17469 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0618 10:16:29.415128   17469 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0618 10:16:29.419854   17469 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jun 18 03:16 /usr/share/ca-certificates/minikubeCA.pem
I0618 10:16:29.419926   17469 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0618 10:16:29.425161   17469 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0618 10:16:29.430436   17469 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0618 10:16:29.432091   17469 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0618 10:16:29.432482   17469 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0618 10:16:29.432594   17469 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0618 10:16:29.447317   17469 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0618 10:16:29.451120   17469 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0618 10:16:29.456647   17469 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0618 10:16:29.458537   17469 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0618 10:16:29.465549   17469 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0618 10:16:29.465912   17469 kubeadm.go:157] found existing configuration files:

I0618 10:16:29.466005   17469 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0618 10:16:29.469803   17469 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0618 10:16:29.469869   17469 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0618 10:16:29.473300   17469 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0618 10:16:29.476891   17469 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0618 10:16:29.476944   17469 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0618 10:16:29.480546   17469 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0618 10:16:29.486201   17469 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0618 10:16:29.486345   17469 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0618 10:16:29.492953   17469 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0618 10:16:29.496985   17469 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0618 10:16:29.497084   17469 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0618 10:16:29.501715   17469 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0618 10:16:29.541555   17469 kubeadm.go:310] [init] Using Kubernetes version: v1.33.1
I0618 10:16:29.541618   17469 kubeadm.go:310] [preflight] Running pre-flight checks
I0618 10:16:29.619382   17469 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0618 10:16:29.619495   17469 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0618 10:16:29.619612   17469 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0618 10:16:29.638961   17469 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0618 10:16:29.649328   17469 out.go:235]     ▪ Generating certificates and keys ...
I0618 10:16:29.649446   17469 kubeadm.go:310] [certs] Using existing ca certificate authority
I0618 10:16:29.649511   17469 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0618 10:16:29.748008   17469 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0618 10:16:30.100700   17469 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0618 10:16:30.653378   17469 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0618 10:16:30.828031   17469 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0618 10:16:31.159556   17469 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0618 10:16:31.159876   17469 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0618 10:16:31.333063   17469 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0618 10:16:31.333252   17469 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0618 10:16:31.422763   17469 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0618 10:16:31.477139   17469 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0618 10:16:31.566449   17469 kubeadm.go:310] [certs] Generating "sa" key and public key
I0618 10:16:31.566523   17469 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0618 10:16:31.718413   17469 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0618 10:16:31.911965   17469 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0618 10:16:32.005301   17469 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0618 10:16:32.120580   17469 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0618 10:16:32.276034   17469 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0618 10:16:32.276895   17469 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0618 10:16:32.280025   17469 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0618 10:16:32.286873   17469 out.go:235]     ▪ Booting up control plane ...
I0618 10:16:32.287989   17469 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0618 10:16:32.288289   17469 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0618 10:16:32.288519   17469 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0618 10:16:32.294047   17469 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0618 10:16:32.297630   17469 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0618 10:16:32.297679   17469 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0618 10:16:32.360736   17469 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0618 10:16:32.360787   17469 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0618 10:16:32.863402   17469 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 502.288792ms
I0618 10:16:32.865173   17469 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0618 10:16:32.865267   17469 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I0618 10:16:32.865363   17469 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0618 10:16:32.865445   17469 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0618 10:16:34.466372   17469 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 1.601036917s
I0618 10:16:35.174217   17469 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 2.308734085s
I0618 10:16:36.369976   17469 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 3.504343835s
I0618 10:16:36.378265   17469 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0618 10:16:36.383622   17469 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0618 10:16:36.390023   17469 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0618 10:16:36.390249   17469 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0618 10:16:36.395612   17469 kubeadm.go:310] [bootstrap-token] Using token: wrj68i.i72nrwl1dqfd7fsf
I0618 10:16:36.399734   17469 out.go:235]     ▪ Configuring RBAC rules ...
I0618 10:16:36.400016   17469 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0618 10:16:36.404491   17469 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0618 10:16:36.406367   17469 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0618 10:16:36.407223   17469 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0618 10:16:36.408131   17469 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0618 10:16:36.408943   17469 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0618 10:16:36.778900   17469 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0618 10:16:37.182133   17469 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0618 10:16:37.777978   17469 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0618 10:16:37.779459   17469 kubeadm.go:310] 
I0618 10:16:37.779568   17469 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0618 10:16:37.779575   17469 kubeadm.go:310] 
I0618 10:16:37.779708   17469 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0618 10:16:37.779724   17469 kubeadm.go:310] 
I0618 10:16:37.779764   17469 kubeadm.go:310]   mkdir -p $HOME/.kube
I0618 10:16:37.779909   17469 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0618 10:16:37.779984   17469 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0618 10:16:37.779990   17469 kubeadm.go:310] 
I0618 10:16:37.780070   17469 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0618 10:16:37.780077   17469 kubeadm.go:310] 
I0618 10:16:37.780152   17469 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0618 10:16:37.780172   17469 kubeadm.go:310] 
I0618 10:16:37.780254   17469 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0618 10:16:37.780369   17469 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0618 10:16:37.780478   17469 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0618 10:16:37.780483   17469 kubeadm.go:310] 
I0618 10:16:37.780615   17469 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0618 10:16:37.780738   17469 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0618 10:16:37.780743   17469 kubeadm.go:310] 
I0618 10:16:37.780901   17469 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token wrj68i.i72nrwl1dqfd7fsf \
I0618 10:16:37.781056   17469 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:a60ffb3ba66a4bc5ba44581a8dd4f49cb5360832cc01191aa33502dd41cdb736 \
I0618 10:16:37.781090   17469 kubeadm.go:310] 	--control-plane 
I0618 10:16:37.781095   17469 kubeadm.go:310] 
I0618 10:16:37.781204   17469 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0618 10:16:37.781208   17469 kubeadm.go:310] 
I0618 10:16:37.781309   17469 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token wrj68i.i72nrwl1dqfd7fsf \
I0618 10:16:37.781443   17469 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:a60ffb3ba66a4bc5ba44581a8dd4f49cb5360832cc01191aa33502dd41cdb736 
I0618 10:16:37.786800   17469 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0618 10:16:37.787081   17469 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0618 10:16:37.787169   17469 cni.go:84] Creating CNI manager for ""
I0618 10:16:37.787219   17469 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0618 10:16:37.794886   17469 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0618 10:16:37.798192   17469 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0618 10:16:37.813272   17469 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0618 10:16:37.827389   17469 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0618 10:16:37.827939   17469 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0618 10:16:37.828517   17469 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_06_18T10_16_37_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0618 10:16:37.834229   17469 ops.go:34] apiserver oom_adj: -16
I0618 10:16:37.896084   17469 kubeadm.go:1105] duration metric: took 68.664042ms to wait for elevateKubeSystemPrivileges
I0618 10:16:37.906518   17469 kubeadm.go:394] duration metric: took 8.4745425s to StartCluster
I0618 10:16:37.906590   17469 settings.go:142] acquiring lock: {Name:mk353ec747b5405c1708cf5634f05e2fdd9f432f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 10:16:37.907342   17469 settings.go:150] Updating kubeconfig:  /Users/didin/.kube/config
I0618 10:16:37.908994   17469 lock.go:35] WriteFile acquiring /Users/didin/.kube/config: {Name:mk886a45c357f9e42ed6dfa6ab2dd15a3c9e0a57 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 10:16:37.910706   17469 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0618 10:16:37.910888   17469 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0618 10:16:37.910979   17469 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0618 10:16:37.911584   17469 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0618 10:16:37.912015   17469 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0618 10:16:37.912017   17469 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0618 10:16:37.912044   17469 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0618 10:16:37.912055   17469 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0618 10:16:37.912789   17469 host.go:66] Checking if "minikube" exists ...
I0618 10:16:37.915685   17469 out.go:177] 🔎  Verifying Kubernetes components...
I0618 10:16:37.916278   17469 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0618 10:16:37.916538   17469 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0618 10:16:37.923580   17469 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0618 10:16:37.949750   17469 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0618 10:16:37.953301   17469 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0618 10:16:37.953327   17469 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0618 10:16:37.953563   17469 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 10:16:37.963833   17469 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0618 10:16:37.963864   17469 host.go:66] Checking if "minikube" exists ...
I0618 10:16:37.964372   17469 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0618 10:16:37.967474   17469 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0618 10:16:37.972606   17469 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0618 10:16:37.973813   17469 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54074 SSHKeyPath:/Users/didin/.minikube/machines/minikube/id_rsa Username:docker}
I0618 10:16:37.982733   17469 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0618 10:16:37.982743   17469 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0618 10:16:37.983031   17469 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 10:16:37.997112   17469 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54074 SSHKeyPath:/Users/didin/.minikube/machines/minikube/id_rsa Username:docker}
I0618 10:16:38.100510   17469 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0618 10:16:38.133056   17469 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0618 10:16:38.139648   17469 start.go:971] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0618 10:16:38.139842   17469 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0618 10:16:38.170052   17469 api_server.go:52] waiting for apiserver process to appear ...
I0618 10:16:38.170172   17469 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0618 10:16:38.298835   17469 api_server.go:72] duration metric: took 387.909791ms to wait for apiserver process to appear ...
I0618 10:16:38.298851   17469 api_server.go:88] waiting for apiserver healthz status ...
I0618 10:16:38.298867   17469 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:54078/healthz ...
I0618 10:16:38.303833   17469 api_server.go:279] https://127.0.0.1:54078/healthz returned 200:
ok
I0618 10:16:38.304990   17469 api_server.go:141] control plane version: v1.33.1
I0618 10:16:38.305007   17469 api_server.go:131] duration metric: took 6.148541ms to wait for apiserver health ...
I0618 10:16:38.305330   17469 system_pods.go:43] waiting for kube-system pods to appear ...
I0618 10:16:38.307375   17469 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0618 10:16:38.312342   17469 addons.go:514] duration metric: took 401.600209ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0618 10:16:38.313048   17469 system_pods.go:59] 5 kube-system pods found
I0618 10:16:38.313072   17469 system_pods.go:61] "etcd-minikube" [23c0c730-7a69-4242-aab1-204789e33864] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0618 10:16:38.313079   17469 system_pods.go:61] "kube-apiserver-minikube" [d54ab9e8-43cb-4636-ad7f-dd3e985c69b3] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0618 10:16:38.313092   17469 system_pods.go:61] "kube-controller-manager-minikube" [0d4425fb-d387-4fa1-b77d-fa2fc4ee4c01] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0618 10:16:38.313106   17469 system_pods.go:61] "kube-scheduler-minikube" [b70b88cd-66da-4d5a-b58d-de2558ffb44c] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0618 10:16:38.313111   17469 system_pods.go:61] "storage-provisioner" [35d57fa5-2dcc-4d0a-89e5-7a71dc74e2e4] Pending
I0618 10:16:38.313116   17469 system_pods.go:74] duration metric: took 7.78175ms to wait for pod list to return data ...
I0618 10:16:38.313124   17469 kubeadm.go:578] duration metric: took 402.202041ms to wait for: map[apiserver:true system_pods:true]
I0618 10:16:38.313134   17469 node_conditions.go:102] verifying NodePressure condition ...
I0618 10:16:38.315025   17469 node_conditions.go:122] node storage ephemeral capacity is 1055761844Ki
I0618 10:16:38.315071   17469 node_conditions.go:123] node cpu capacity is 8
I0618 10:16:38.315509   17469 node_conditions.go:105] duration metric: took 2.152042ms to run NodePressure ...
I0618 10:16:38.315521   17469 start.go:241] waiting for startup goroutines ...
I0618 10:16:38.650311   17469 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0618 10:16:38.650367   17469 start.go:246] waiting for cluster config update ...
I0618 10:16:38.650389   17469 start.go:255] writing updated cluster config ...
I0618 10:16:38.656970   17469 ssh_runner.go:195] Run: rm -f paused
I0618 10:16:38.966530   17469 start.go:607] kubectl: 1.32.2, cluster: 1.33.1 (minor skew: 1)
I0618 10:16:38.970730   17469 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jun 18 03:16:25 minikube dockerd[1022]: time="2025-06-18T03:16:25.998279667Z" level=info msg="Starting up"
Jun 18 03:16:26 minikube dockerd[1022]: time="2025-06-18T03:16:26.000221875Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Jun 18 03:16:26 minikube dockerd[1022]: time="2025-06-18T03:16:26.004382834Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Jun 18 03:16:26 minikube dockerd[1022]: time="2025-06-18T03:16:26.021006209Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Jun 18 03:16:26 minikube dockerd[1022]: time="2025-06-18T03:16:26.022738000Z" level=info msg="Loading containers: start."
Jun 18 03:16:26 minikube dockerd[1022]: time="2025-06-18T03:16:26.115798459Z" level=info msg="Processing signal 'terminated'"
Jun 18 03:16:26 minikube dockerd[1022]: time="2025-06-18T03:16:26.679539417Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count dbe91588333018663bf71eddf10831f43ba6ce02ed5243f88bd42935783226d4], retrying...."
Jun 18 03:16:26 minikube dockerd[1022]: time="2025-06-18T03:16:26.703479126Z" level=info msg="Loading containers: done."
Jun 18 03:16:26 minikube dockerd[1022]: time="2025-06-18T03:16:26.712483209Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Jun 18 03:16:26 minikube dockerd[1022]: time="2025-06-18T03:16:26.712522209Z" level=info msg="Initializing buildkit"
Jun 18 03:16:26 minikube dockerd[1022]: time="2025-06-18T03:16:26.727001917Z" level=info msg="Completed buildkit initialization"
Jun 18 03:16:26 minikube dockerd[1022]: time="2025-06-18T03:16:26.732007084Z" level=info msg="Daemon has completed initialization"
Jun 18 03:16:26 minikube dockerd[1022]: time="2025-06-18T03:16:26.732055542Z" level=info msg="API listen on /var/run/docker.sock"
Jun 18 03:16:26 minikube dockerd[1022]: time="2025-06-18T03:16:26.732089751Z" level=info msg="API listen on [::]:2376"
Jun 18 03:16:26 minikube dockerd[1022]: time="2025-06-18T03:16:26.732533001Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Jun 18 03:16:26 minikube dockerd[1022]: time="2025-06-18T03:16:26.732766001Z" level=info msg="Daemon shutdown complete"
Jun 18 03:16:26 minikube systemd[1]: docker.service: Deactivated successfully.
Jun 18 03:16:26 minikube systemd[1]: Stopped Docker Application Container Engine.
Jun 18 03:16:26 minikube systemd[1]: Starting Docker Application Container Engine...
Jun 18 03:16:26 minikube dockerd[1330]: time="2025-06-18T03:16:26.767039626Z" level=info msg="Starting up"
Jun 18 03:16:26 minikube dockerd[1330]: time="2025-06-18T03:16:26.770423917Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Jun 18 03:16:26 minikube dockerd[1330]: time="2025-06-18T03:16:26.776374292Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Jun 18 03:16:26 minikube dockerd[1330]: time="2025-06-18T03:16:26.782578251Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jun 18 03:16:26 minikube dockerd[1330]: time="2025-06-18T03:16:26.793774626Z" level=info msg="Loading containers: start."
Jun 18 03:16:27 minikube dockerd[1330]: time="2025-06-18T03:16:27.557777543Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 8baab3aa3893ccf1c59a134eb46d100e1f39b2109a8378966401a801a3015eb6], retrying...."
Jun 18 03:16:27 minikube dockerd[1330]: time="2025-06-18T03:16:27.581790084Z" level=info msg="Loading containers: done."
Jun 18 03:16:27 minikube dockerd[1330]: time="2025-06-18T03:16:27.596682543Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Jun 18 03:16:27 minikube dockerd[1330]: time="2025-06-18T03:16:27.596737584Z" level=info msg="Initializing buildkit"
Jun 18 03:16:27 minikube dockerd[1330]: time="2025-06-18T03:16:27.611774918Z" level=info msg="Completed buildkit initialization"
Jun 18 03:16:27 minikube dockerd[1330]: time="2025-06-18T03:16:27.616454876Z" level=info msg="Daemon has completed initialization"
Jun 18 03:16:27 minikube dockerd[1330]: time="2025-06-18T03:16:27.616509459Z" level=info msg="API listen on [::]:2376"
Jun 18 03:16:27 minikube systemd[1]: Started Docker Application Container Engine.
Jun 18 03:16:27 minikube dockerd[1330]: time="2025-06-18T03:16:27.616564209Z" level=info msg="API listen on /var/run/docker.sock"
Jun 18 03:16:27 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jun 18 03:16:27 minikube cri-dockerd[1638]: time="2025-06-18T03:16:27Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jun 18 03:16:27 minikube cri-dockerd[1638]: time="2025-06-18T03:16:27Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jun 18 03:16:27 minikube cri-dockerd[1638]: time="2025-06-18T03:16:27Z" level=info msg="Start docker client with request timeout 0s"
Jun 18 03:16:27 minikube cri-dockerd[1638]: time="2025-06-18T03:16:27Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jun 18 03:16:27 minikube cri-dockerd[1638]: time="2025-06-18T03:16:27Z" level=info msg="Loaded network plugin cni"
Jun 18 03:16:27 minikube cri-dockerd[1638]: time="2025-06-18T03:16:27Z" level=info msg="Docker cri networking managed by network plugin cni"
Jun 18 03:16:27 minikube cri-dockerd[1638]: time="2025-06-18T03:16:27Z" level=info msg="Setting cgroupDriver cgroupfs"
Jun 18 03:16:27 minikube cri-dockerd[1638]: time="2025-06-18T03:16:27Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jun 18 03:16:27 minikube cri-dockerd[1638]: time="2025-06-18T03:16:27Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jun 18 03:16:27 minikube cri-dockerd[1638]: time="2025-06-18T03:16:27Z" level=info msg="Start cri-dockerd grpc backend"
Jun 18 03:16:27 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jun 18 03:16:33 minikube cri-dockerd[1638]: time="2025-06-18T03:16:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/05b590fd8d021cdd312efe5722dcd335245ba19e49c656a18c924ed448bd771e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 18 03:16:33 minikube cri-dockerd[1638]: time="2025-06-18T03:16:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/539f7a8a2057d80fd93f785b3e0da2e3849f9abb7bf5990d8bc1855725552581/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 18 03:16:33 minikube cri-dockerd[1638]: time="2025-06-18T03:16:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c02be0b434e616645842ee216dd65757a843d52baf9edb8e50ef6a5c32bb3b53/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 18 03:16:33 minikube cri-dockerd[1638]: time="2025-06-18T03:16:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/428abf6aeeb330cd70b3be516a9e3f36a0e645f2243c77f429cdb0360165d95f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 18 03:16:43 minikube cri-dockerd[1638]: time="2025-06-18T03:16:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/618c70dde4e44a9613e05afe3edcfa6f9d02f8ce96b0006b64d727d4cfc50ece/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 18 03:16:43 minikube cri-dockerd[1638]: time="2025-06-18T03:16:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d398f413ca7cd58ef75f7c343c885f55b1a1926d2f97f068a758cd917f99c483/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 18 03:16:44 minikube cri-dockerd[1638]: time="2025-06-18T03:16:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6c8ecfcf3bb03aa6975e5bb26b7cbf9dffcbf0a2aca8ebda329f92a37c38bd9a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 18 03:16:47 minikube cri-dockerd[1638]: time="2025-06-18T03:16:47Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jun 18 03:17:16 minikube cri-dockerd[1638]: time="2025-06-18T03:17:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8766e325086255d016817f5628eadcb342b070ace76e51b5d5237861bca6f492/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 18 03:17:19 minikube dockerd[1330]: time="2025-06-18T03:17:19.757299428Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 18 03:17:19 minikube dockerd[1330]: time="2025-06-18T03:17:19.757445095Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 18 03:17:38 minikube dockerd[1330]: time="2025-06-18T03:17:38.027686589Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 18 03:17:38 minikube dockerd[1330]: time="2025-06-18T03:17:38.027946173Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 18 03:18:09 minikube dockerd[1330]: time="2025-06-18T03:18:09.982384257Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jun 18 03:18:09 minikube dockerd[1330]: time="2025-06-18T03:18:09.982642632Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
93633b470d51d       ba04bb24b9575       2 minutes ago       Running             storage-provisioner       0                   6c8ecfcf3bb03       storage-provisioner
a54da078bfb9a       f72407be9e08c       2 minutes ago       Running             coredns                   0                   d398f413ca7cd       coredns-674b8bbfcf-bz55r
c2b6315b8bdcd       3e58848989f55       2 minutes ago       Running             kube-proxy                0                   618c70dde4e44       kube-proxy-d5lkz
6bce9371d6429       014094c90caac       2 minutes ago       Running             kube-scheduler            0                   428abf6aeeb33       kube-scheduler-minikube
3e83c67709d0f       31747a36ce712       2 minutes ago       Running             etcd                      0                   c02be0b434e61       etcd-minikube
0b789aa1cdf47       9a2b7cf4f8540       2 minutes ago       Running             kube-apiserver            0                   539f7a8a2057d       kube-apiserver-minikube
e442120c94524       674996a72aa59       2 minutes ago       Running             kube-controller-manager   0                   05b590fd8d021       kube-controller-manager-minikube


==> coredns [a54da078bfb9] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.0
linux/arm64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:46841 - 8423 "HINFO IN 7098805170039414544.2172637739009759310. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.110304083s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_06_18T10_16_37_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 18 Jun 2025 03:16:35 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 18 Jun 2025 03:18:59 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 18 Jun 2025 03:16:47 +0000   Wed, 18 Jun 2025 03:16:34 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 18 Jun 2025 03:16:47 +0000   Wed, 18 Jun 2025 03:16:34 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 18 Jun 2025 03:16:47 +0000   Wed, 18 Jun 2025 03:16:34 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 18 Jun 2025 03:16:47 +0000   Wed, 18 Jun 2025 03:16:35 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055761844Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             4013480Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055761844Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             4013480Ki
  pods:               110
System Info:
  Machine ID:                 c80e9b0ce3d04aaabc2b065b12b1631c
  System UUID:                c80e9b0ce3d04aaabc2b065b12b1631c
  Boot ID:                    2196c219-a8d9-4ea6-82c9-12c444c7c624
  Kernel Version:             6.10.14-linuxkit
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     quarkus-demo-69c67dc5dd-nspxp       0 (0%)        0 (0%)      0 (0%)           0 (0%)         105s
  kube-system                 coredns-674b8bbfcf-bz55r            100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     2m17s
  kube-system                 etcd-minikube                       100m (1%)     0 (0%)      100Mi (2%)       0 (0%)         2m23s
  kube-system                 kube-apiserver-minikube             250m (3%)     0 (0%)      0 (0%)           0 (0%)         2m24s
  kube-system                 kube-controller-manager-minikube    200m (2%)     0 (0%)      0 (0%)           0 (0%)         2m23s
  kube-system                 kube-proxy-d5lkz                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m18s
  kube-system                 kube-scheduler-minikube             100m (1%)     0 (0%)      0 (0%)           0 (0%)         2m23s
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m22s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
  hugepages-32Mi     0 (0%)      0 (0%)
  hugepages-64Ki     0 (0%)      0 (0%)
Events:
  Type    Reason                   Age    From             Message
  ----    ------                   ----   ----             -------
  Normal  Starting                 2m17s  kube-proxy       
  Normal  Starting                 2m24s  kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  2m24s  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  2m23s  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    2m23s  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     2m23s  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           2m19s  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Jun18 03:03] netlink: 'init': attribute type 4 has an invalid length.
[  +0.029466] fakeowner: loading out-of-tree module taints kernel.


==> etcd [3e83c67709d0] <==
{"level":"warn","ts":"2025-06-18T03:16:33.654368Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"warn","ts":"2025-06-18T03:16:33.654509Z","caller":"etcdmain/config.go:389","msg":"--proxy-refresh-interval is deprecated in 3.5 and will be decommissioned in 3.6."}
{"level":"info","ts":"2025-06-18T03:16:33.654522Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-06-18T03:16:33.654567Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-06-18T03:16:33.654578Z","caller":"embed/etcd.go:140","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-06-18T03:16:33.654591Z","caller":"embed/etcd.go:528","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-06-18T03:16:33.655482Z","caller":"embed/etcd.go:148","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-06-18T03:16:33.655698Z","caller":"embed/etcd.go:323","msg":"starting an etcd server","etcd-version":"3.5.21","git-sha":"a17edfd59","go-version":"go1.23.7","go-os":"linux","go-arch":"arm64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-06-18T03:16:33.658038Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.81625ms"}
{"level":"info","ts":"2025-06-18T03:16:33.666211Z","caller":"etcdserver/raft.go:506","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2025-06-18T03:16:33.666339Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-06-18T03:16:33.666394Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-06-18T03:16:33.666409Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-06-18T03:16:33.666422Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-06-18T03:16:33.666445Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-06-18T03:16:33.667519Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-06-18T03:16:33.669740Z","caller":"mvcc/kvstore.go:425","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-06-18T03:16:33.669768Z","caller":"etcdserver/server.go:628","msg":"restore consistentIndex","index":0}
{"level":"info","ts":"2025-06-18T03:16:33.670320Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-06-18T03:16:33.671064Z","caller":"etcdserver/server.go:875","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.21","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-06-18T03:16:33.671295Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-18T03:16:33.671471Z","caller":"etcdserver/server.go:759","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-06-18T03:16:33.671570Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-18T03:16:33.671708Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-18T03:16:33.671716Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-18T03:16:33.671881Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-06-18T03:16:33.671922Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-06-18T03:16:33.672151Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-06-18T03:16:33.672218Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-18T03:16:33.672232Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-18T03:16:33.672317Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-06-18T03:16:33.672706Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-06-18T03:16:34.566663Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-06-18T03:16:34.566701Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-06-18T03:16:34.566745Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-06-18T03:16:34.566764Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-06-18T03:16:34.566767Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-06-18T03:16:34.566776Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-06-18T03:16:34.566781Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-06-18T03:16:34.567139Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-06-18T03:16:34.567158Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-18T03:16:34.567332Z","caller":"etcdserver/server.go:2697","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-18T03:16:34.567627Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-18T03:16:34.567979Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-06-18T03:16:34.568088Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-06-18T03:16:34.568122Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-18T03:16:34.568183Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-18T03:16:34.568228Z","caller":"etcdserver/server.go:2721","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-18T03:16:34.568240Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-18T03:16:34.568520Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-18T03:16:34.568761Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-06-18T03:16:34.568897Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-06-18T03:17:08.563992Z","caller":"traceutil/trace.go:171","msg":"trace[1098451889] transaction","detail":"{read_only:false; response_revision:405; number_of_response:1; }","duration":"161.26175ms","start":"2025-06-18T03:17:08.402491Z","end":"2025-06-18T03:17:08.563752Z","steps":["trace[1098451889] 'process raft request'  (duration: 160.263667ms)"],"step_count":1}


==> kernel <==
 03:19:00 up 15 min,  0 users,  load average: 3.57, 1.63, 0.67
Linux minikube 6.10.14-linuxkit #1 SMP Tue Apr 15 16:00:54 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [0b789aa1cdf4] <==
I0618 03:16:35.069548       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0618 03:16:35.069570       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0618 03:16:35.069774       1 controller.go:78] Starting OpenAPI AggregationController
I0618 03:16:35.069991       1 controller.go:142] Starting OpenAPI controller
I0618 03:16:35.070001       1 controller.go:90] Starting OpenAPI V3 controller
I0618 03:16:35.070017       1 naming_controller.go:299] Starting NamingConditionController
I0618 03:16:35.070025       1 establishing_controller.go:81] Starting EstablishingController
I0618 03:16:35.070045       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0618 03:16:35.070054       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0618 03:16:35.070059       1 crd_finalizer.go:269] Starting CRDFinalizer
I0618 03:16:35.073129       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0618 03:16:35.073168       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0618 03:16:35.073211       1 repairip.go:200] Starting ipallocator-repair-controller
I0618 03:16:35.073225       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0618 03:16:35.168850       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0618 03:16:35.168906       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0618 03:16:35.168936       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0618 03:16:35.169172       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0618 03:16:35.169562       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0618 03:16:35.169947       1 aggregator.go:171] initial CRD sync complete...
I0618 03:16:35.170001       1 autoregister_controller.go:144] Starting autoregister controller
I0618 03:16:35.170007       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0618 03:16:35.170010       1 cache.go:39] Caches are synced for autoregister controller
I0618 03:16:35.170731       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0618 03:16:35.170801       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0618 03:16:35.170804       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0618 03:16:35.170834       1 cache.go:39] Caches are synced for LocalAvailability controller
I0618 03:16:35.173258       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0618 03:16:35.173281       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0618 03:16:35.173291       1 default_servicecidr_controller.go:165] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I0618 03:16:35.188861       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
E0618 03:16:35.200260       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
E0618 03:16:35.221631       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I0618 03:16:35.236776       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0618 03:16:35.236804       1 policy_source.go:240] refreshing policies
I0618 03:16:35.272741       1 controller.go:667] quota admission added evaluator for: namespaces
I0618 03:16:35.276433       1 default_servicecidr_controller.go:214] Setting default ServiceCIDR condition Ready to True
I0618 03:16:35.276486       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0618 03:16:35.280243       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0618 03:16:35.280958       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0618 03:16:35.404966       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0618 03:16:36.076394       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0618 03:16:36.080203       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0618 03:16:36.080222       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0618 03:16:36.225788       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0618 03:16:36.235658       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0618 03:16:36.273053       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0618 03:16:36.274624       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0618 03:16:36.274904       1 controller.go:667] quota admission added evaluator for: endpoints
I0618 03:16:36.275868       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0618 03:16:37.152061       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0618 03:16:37.176751       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0618 03:16:37.181078       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0618 03:16:37.184912       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0618 03:16:42.359846       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I0618 03:16:42.551866       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0618 03:16:42.706424       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0618 03:16:42.708281       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0618 03:17:21.498058       1 alloc.go:328] "allocated clusterIPs" service="default/quarkus-demo" clusterIPs={"IPv4":"10.110.208.203"}
I0618 03:17:21.502111       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-controller-manager [e442120c9452] <==
I0618 03:16:41.603950       1 controllermanager.go:778] "Started controller" controller="persistentvolumeclaim-protection-controller"
I0618 03:16:41.604036       1 pvc_protection_controller.go:168] "Starting PVC protection controller" logger="persistentvolumeclaim-protection-controller"
I0618 03:16:41.604052       1 shared_informer.go:350] "Waiting for caches to sync" controller="PVC protection"
I0618 03:16:41.754875       1 controllermanager.go:778] "Started controller" controller="ttl-after-finished-controller"
I0618 03:16:41.755708       1 ttlafterfinished_controller.go:112] "Starting TTL after finished controller" logger="ttl-after-finished-controller"
I0618 03:16:41.755737       1 shared_informer.go:350] "Waiting for caches to sync" controller="TTL after finished"
I0618 03:16:41.767390       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0618 03:16:41.786159       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0618 03:16:41.834708       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0618 03:16:41.834727       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0618 03:16:41.834954       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0618 03:16:41.835037       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0618 03:16:41.835468       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0618 03:16:41.835537       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0618 03:16:41.835587       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0618 03:16:41.835607       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0618 03:16:41.842691       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0618 03:16:41.850845       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0618 03:16:41.851834       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0618 03:16:41.852004       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0618 03:16:41.852018       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0618 03:16:41.852027       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0618 03:16:41.854025       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0618 03:16:41.854040       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0618 03:16:41.854091       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0618 03:16:41.854099       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0618 03:16:41.854167       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0618 03:16:41.857140       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0618 03:16:41.863857       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0618 03:16:41.869850       1 shared_informer.go:357] "Caches are synced" controller="job"
I0618 03:16:41.875827       1 shared_informer.go:357] "Caches are synced" controller="node"
I0618 03:16:41.875869       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0618 03:16:41.875893       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0618 03:16:41.875900       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0618 03:16:41.875912       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0618 03:16:41.877821       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0618 03:16:41.880775       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0618 03:16:41.881239       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0618 03:16:41.884891       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0618 03:16:41.885823       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0618 03:16:41.885885       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0618 03:16:41.885972       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0618 03:16:41.886010       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0618 03:16:41.904852       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0618 03:16:42.000690       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0618 03:16:42.002231       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0618 03:16:42.104872       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0618 03:16:42.107045       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0618 03:16:42.113102       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0618 03:16:42.149813       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0618 03:16:42.155345       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0618 03:16:42.155381       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0618 03:16:42.156939       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0618 03:16:42.160397       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0618 03:16:42.162017       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0618 03:16:42.168901       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0618 03:16:42.643106       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0618 03:16:42.660138       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0618 03:16:42.660167       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0618 03:16:42.660173       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-proxy [c2b6315b8bdc] <==
I0618 03:16:43.506782       1 server_linux.go:63] "Using iptables proxy"
I0618 03:16:43.590098       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0618 03:16:43.590137       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0618 03:16:43.606440       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0618 03:16:43.606471       1 server_linux.go:145] "Using iptables Proxier"
I0618 03:16:43.609189       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0618 03:16:43.609403       1 server.go:516] "Version info" version="v1.33.1"
I0618 03:16:43.609413       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0618 03:16:43.610286       1 config.go:199] "Starting service config controller"
I0618 03:16:43.610307       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0618 03:16:43.610320       1 config.go:105] "Starting endpoint slice config controller"
I0618 03:16:43.610326       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0618 03:16:43.610331       1 config.go:440] "Starting serviceCIDR config controller"
I0618 03:16:43.610333       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0618 03:16:43.611209       1 config.go:329] "Starting node config controller"
I0618 03:16:43.611219       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0618 03:16:43.711665       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0618 03:16:43.711666       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0618 03:16:43.711683       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0618 03:16:43.711690       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"


==> kube-scheduler [6bce9371d642] <==
I0618 03:16:33.838635       1 serving.go:386] Generated self-signed cert in-memory
W0618 03:16:35.147749       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0618 03:16:35.148003       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0618 03:16:35.148020       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0618 03:16:35.148029       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0618 03:16:35.161883       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0618 03:16:35.161904       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0618 03:16:35.164869       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0618 03:16:35.165329       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0618 03:16:35.166054       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0618 03:16:35.166951       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0618 03:16:35.171432       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0618 03:16:35.171479       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0618 03:16:35.171521       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0618 03:16:35.171673       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0618 03:16:35.171685       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0618 03:16:35.171694       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0618 03:16:35.171703       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0618 03:16:35.171799       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0618 03:16:35.171810       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0618 03:16:35.171822       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0618 03:16:35.171874       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_arm64.s:1223" type="*v1.ConfigMap"
E0618 03:16:35.172950       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0618 03:16:35.172984       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0618 03:16:35.173017       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0618 03:16:35.173052       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0618 03:16:35.173418       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0618 03:16:36.003004       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0618 03:16:36.097680       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0618 03:16:36.208112       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_arm64.s:1223" type="*v1.ConfigMap"
I0618 03:16:38.166744       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.218372    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.218386    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/3924ef3609584191d8d09190210d2d78-etcd-certs\") pod \"etcd-minikube\" (UID: \"3924ef3609584191d8d09190210d2d78\") " pod="kube-system/etcd-minikube"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.218397    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.218408    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.218426    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/3924ef3609584191d8d09190210d2d78-etcd-data\") pod \"etcd-minikube\" (UID: \"3924ef3609584191d8d09190210d2d78\") " pod="kube-system/etcd-minikube"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.218437    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.218446    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.218459    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.218468    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.218477    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/feee622ba49882ef945e2406d3ba86df-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"feee622ba49882ef945e2406d3ba86df\") " pod="kube-system/kube-scheduler-minikube"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.218486    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.911083    2486 apiserver.go:52] "Watching apiserver"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.915792    2486 desired_state_of_world_populator.go:158] "Finished populating initial desired state of world"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.944854    2486 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Jun 18 03:16:37 minikube kubelet[2486]: E0618 03:16:37.956428    2486 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.956657    2486 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.9566294640000002 podStartE2EDuration="1.956629464s" podCreationTimestamp="2025-06-18 03:16:36 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-18 03:16:37.956197381 +0000 UTC m=+1.079082585" watchObservedRunningTime="2025-06-18 03:16:37.956629464 +0000 UTC m=+1.079514710"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.968718    2486 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=0.968695464 podStartE2EDuration="968.695464ms" podCreationTimestamp="2025-06-18 03:16:37 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-18 03:16:37.962326423 +0000 UTC m=+1.085211668" watchObservedRunningTime="2025-06-18 03:16:37.968695464 +0000 UTC m=+1.091580710"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.975846    2486 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=0.975823756 podStartE2EDuration="975.823756ms" podCreationTimestamp="2025-06-18 03:16:37 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-18 03:16:37.968920256 +0000 UTC m=+1.091805502" watchObservedRunningTime="2025-06-18 03:16:37.975823756 +0000 UTC m=+1.098709043"
Jun 18 03:16:37 minikube kubelet[2486]: I0618 03:16:37.975996    2486 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=0.975988631 podStartE2EDuration="975.988631ms" podCreationTimestamp="2025-06-18 03:16:37 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-18 03:16:37.975474756 +0000 UTC m=+1.098360043" watchObservedRunningTime="2025-06-18 03:16:37.975988631 +0000 UTC m=+1.098873877"
Jun 18 03:16:41 minikube kubelet[2486]: I0618 03:16:41.975561    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/35d57fa5-2dcc-4d0a-89e5-7a71dc74e2e4-tmp\") pod \"storage-provisioner\" (UID: \"35d57fa5-2dcc-4d0a-89e5-7a71dc74e2e4\") " pod="kube-system/storage-provisioner"
Jun 18 03:16:41 minikube kubelet[2486]: I0618 03:16:41.975594    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qpz5c\" (UniqueName: \"kubernetes.io/projected/35d57fa5-2dcc-4d0a-89e5-7a71dc74e2e4-kube-api-access-qpz5c\") pod \"storage-provisioner\" (UID: \"35d57fa5-2dcc-4d0a-89e5-7a71dc74e2e4\") " pod="kube-system/storage-provisioner"
Jun 18 03:16:42 minikube kubelet[2486]: E0618 03:16:42.081827    2486 projected.go:289] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Jun 18 03:16:42 minikube kubelet[2486]: E0618 03:16:42.081869    2486 projected.go:194] Error preparing data for projected volume kube-api-access-qpz5c for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Jun 18 03:16:42 minikube kubelet[2486]: E0618 03:16:42.081957    2486 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/35d57fa5-2dcc-4d0a-89e5-7a71dc74e2e4-kube-api-access-qpz5c podName:35d57fa5-2dcc-4d0a-89e5-7a71dc74e2e4 nodeName:}" failed. No retries permitted until 2025-06-18 03:16:42.581928341 +0000 UTC m=+5.704813587 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-qpz5c" (UniqueName: "kubernetes.io/projected/35d57fa5-2dcc-4d0a-89e5-7a71dc74e2e4-kube-api-access-qpz5c") pod "storage-provisioner" (UID: "35d57fa5-2dcc-4d0a-89e5-7a71dc74e2e4") : configmap "kube-root-ca.crt" not found
Jun 18 03:16:42 minikube kubelet[2486]: I0618 03:16:42.581169    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2vjdn\" (UniqueName: \"kubernetes.io/projected/d0a24ac5-8eea-4d66-8bfb-91336783b86f-kube-api-access-2vjdn\") pod \"kube-proxy-d5lkz\" (UID: \"d0a24ac5-8eea-4d66-8bfb-91336783b86f\") " pod="kube-system/kube-proxy-d5lkz"
Jun 18 03:16:42 minikube kubelet[2486]: I0618 03:16:42.581249    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/d0a24ac5-8eea-4d66-8bfb-91336783b86f-lib-modules\") pod \"kube-proxy-d5lkz\" (UID: \"d0a24ac5-8eea-4d66-8bfb-91336783b86f\") " pod="kube-system/kube-proxy-d5lkz"
Jun 18 03:16:42 minikube kubelet[2486]: I0618 03:16:42.581280    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/d0a24ac5-8eea-4d66-8bfb-91336783b86f-kube-proxy\") pod \"kube-proxy-d5lkz\" (UID: \"d0a24ac5-8eea-4d66-8bfb-91336783b86f\") " pod="kube-system/kube-proxy-d5lkz"
Jun 18 03:16:42 minikube kubelet[2486]: I0618 03:16:42.581308    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/d0a24ac5-8eea-4d66-8bfb-91336783b86f-xtables-lock\") pod \"kube-proxy-d5lkz\" (UID: \"d0a24ac5-8eea-4d66-8bfb-91336783b86f\") " pod="kube-system/kube-proxy-d5lkz"
Jun 18 03:16:42 minikube kubelet[2486]: E0618 03:16:42.683411    2486 projected.go:289] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Jun 18 03:16:42 minikube kubelet[2486]: E0618 03:16:42.683469    2486 projected.go:194] Error preparing data for projected volume kube-api-access-qpz5c for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Jun 18 03:16:42 minikube kubelet[2486]: E0618 03:16:42.683592    2486 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/35d57fa5-2dcc-4d0a-89e5-7a71dc74e2e4-kube-api-access-qpz5c podName:35d57fa5-2dcc-4d0a-89e5-7a71dc74e2e4 nodeName:}" failed. No retries permitted until 2025-06-18 03:16:43.683564716 +0000 UTC m=+6.806450045 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "kube-api-access-qpz5c" (UniqueName: "kubernetes.io/projected/35d57fa5-2dcc-4d0a-89e5-7a71dc74e2e4-kube-api-access-qpz5c") pod "storage-provisioner" (UID: "35d57fa5-2dcc-4d0a-89e5-7a71dc74e2e4") : configmap "kube-root-ca.crt" not found
Jun 18 03:16:42 minikube kubelet[2486]: E0618 03:16:42.688303    2486 projected.go:289] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Jun 18 03:16:42 minikube kubelet[2486]: E0618 03:16:42.688333    2486 projected.go:194] Error preparing data for projected volume kube-api-access-2vjdn for pod kube-system/kube-proxy-d5lkz: configmap "kube-root-ca.crt" not found
Jun 18 03:16:42 minikube kubelet[2486]: E0618 03:16:42.688410    2486 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/d0a24ac5-8eea-4d66-8bfb-91336783b86f-kube-api-access-2vjdn podName:d0a24ac5-8eea-4d66-8bfb-91336783b86f nodeName:}" failed. No retries permitted until 2025-06-18 03:16:43.188386216 +0000 UTC m=+6.311271462 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-2vjdn" (UniqueName: "kubernetes.io/projected/d0a24ac5-8eea-4d66-8bfb-91336783b86f-kube-api-access-2vjdn") pod "kube-proxy-d5lkz" (UID: "d0a24ac5-8eea-4d66-8bfb-91336783b86f") : configmap "kube-root-ca.crt" not found
Jun 18 03:16:43 minikube kubelet[2486]: I0618 03:16:43.090192    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-n6jpw\" (UniqueName: \"kubernetes.io/projected/3ebfc1b4-2cd6-43c4-ba70-8fb053cec5bf-kube-api-access-n6jpw\") pod \"coredns-674b8bbfcf-bz55r\" (UID: \"3ebfc1b4-2cd6-43c4-ba70-8fb053cec5bf\") " pod="kube-system/coredns-674b8bbfcf-bz55r"
Jun 18 03:16:43 minikube kubelet[2486]: I0618 03:16:43.090240    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/3ebfc1b4-2cd6-43c4-ba70-8fb053cec5bf-config-volume\") pod \"coredns-674b8bbfcf-bz55r\" (UID: \"3ebfc1b4-2cd6-43c4-ba70-8fb053cec5bf\") " pod="kube-system/coredns-674b8bbfcf-bz55r"
Jun 18 03:16:43 minikube kubelet[2486]: I0618 03:16:43.992405    2486 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-674b8bbfcf-bz55r" podStartSLOduration=0.992368342 podStartE2EDuration="992.368342ms" podCreationTimestamp="2025-06-18 03:16:43 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-18 03:16:43.992151467 +0000 UTC m=+7.115036754" watchObservedRunningTime="2025-06-18 03:16:43.992368342 +0000 UTC m=+7.115253588"
Jun 18 03:16:43 minikube kubelet[2486]: I0618 03:16:43.996404    2486 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-d5lkz" podStartSLOduration=1.996397134 podStartE2EDuration="1.996397134s" podCreationTimestamp="2025-06-18 03:16:42 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-18 03:16:43.996219967 +0000 UTC m=+7.119105213" watchObservedRunningTime="2025-06-18 03:16:43.996397134 +0000 UTC m=+7.119282379"
Jun 18 03:16:45 minikube kubelet[2486]: I0618 03:16:45.015912    2486 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=7.015880384 podStartE2EDuration="7.015880384s" podCreationTimestamp="2025-06-18 03:16:38 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-18 03:16:45.015767426 +0000 UTC m=+8.138652755" watchObservedRunningTime="2025-06-18 03:16:45.015880384 +0000 UTC m=+8.138765630"
Jun 18 03:16:47 minikube kubelet[2486]: I0618 03:16:47.326576    2486 kuberuntime_manager.go:1746] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jun 18 03:16:47 minikube kubelet[2486]: I0618 03:16:47.328410    2486 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jun 18 03:16:49 minikube kubelet[2486]: I0618 03:16:49.586924    2486 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Jun 18 03:17:15 minikube kubelet[2486]: I0618 03:17:15.831647    2486 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-czb7c\" (UniqueName: \"kubernetes.io/projected/87df75eb-ded5-4e3d-8110-642fb6ed719f-kube-api-access-czb7c\") pod \"quarkus-demo-69c67dc5dd-nspxp\" (UID: \"87df75eb-ded5-4e3d-8110-642fb6ed719f\") " pod="default/quarkus-demo-69c67dc5dd-nspxp"
Jun 18 03:17:19 minikube kubelet[2486]: E0618 03:17:19.761139    2486 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="yourusername/quarkus-demo:latest"
Jun 18 03:17:19 minikube kubelet[2486]: E0618 03:17:19.761305    2486 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="yourusername/quarkus-demo:latest"
Jun 18 03:17:19 minikube kubelet[2486]: E0618 03:17:19.761945    2486 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:quarkus-demo,Image:yourusername/quarkus-demo,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-czb7c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod quarkus-demo-69c67dc5dd-nspxp_default(87df75eb-ded5-4e3d-8110-642fb6ed719f): ErrImagePull: Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jun 18 03:17:19 minikube kubelet[2486]: E0618 03:17:19.763663    2486 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"quarkus-demo\" with ErrImagePull: \"Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/quarkus-demo-69c67dc5dd-nspxp" podUID="87df75eb-ded5-4e3d-8110-642fb6ed719f"
Jun 18 03:17:20 minikube kubelet[2486]: E0618 03:17:20.285126    2486 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"quarkus-demo\" with ImagePullBackOff: \"Back-off pulling image \\\"yourusername/quarkus-demo\\\": ErrImagePull: Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/quarkus-demo-69c67dc5dd-nspxp" podUID="87df75eb-ded5-4e3d-8110-642fb6ed719f"
Jun 18 03:17:38 minikube kubelet[2486]: E0618 03:17:38.049099    2486 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="yourusername/quarkus-demo:latest"
Jun 18 03:17:38 minikube kubelet[2486]: E0618 03:17:38.049220    2486 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="yourusername/quarkus-demo:latest"
Jun 18 03:17:38 minikube kubelet[2486]: E0618 03:17:38.049483    2486 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:quarkus-demo,Image:yourusername/quarkus-demo,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-czb7c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod quarkus-demo-69c67dc5dd-nspxp_default(87df75eb-ded5-4e3d-8110-642fb6ed719f): ErrImagePull: Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jun 18 03:17:38 minikube kubelet[2486]: E0618 03:17:38.050920    2486 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"quarkus-demo\" with ErrImagePull: \"Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/quarkus-demo-69c67dc5dd-nspxp" podUID="87df75eb-ded5-4e3d-8110-642fb6ed719f"
Jun 18 03:17:51 minikube kubelet[2486]: E0618 03:17:51.925797    2486 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"quarkus-demo\" with ImagePullBackOff: \"Back-off pulling image \\\"yourusername/quarkus-demo\\\": ErrImagePull: Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/quarkus-demo-69c67dc5dd-nspxp" podUID="87df75eb-ded5-4e3d-8110-642fb6ed719f"
Jun 18 03:18:09 minikube kubelet[2486]: E0618 03:18:09.986571    2486 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="yourusername/quarkus-demo:latest"
Jun 18 03:18:09 minikube kubelet[2486]: E0618 03:18:09.986753    2486 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="yourusername/quarkus-demo:latest"
Jun 18 03:18:09 minikube kubelet[2486]: E0618 03:18:09.987862    2486 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:quarkus-demo,Image:yourusername/quarkus-demo,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-czb7c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod quarkus-demo-69c67dc5dd-nspxp_default(87df75eb-ded5-4e3d-8110-642fb6ed719f): ErrImagePull: Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jun 18 03:18:09 minikube kubelet[2486]: E0618 03:18:09.989691    2486 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"quarkus-demo\" with ErrImagePull: \"Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/quarkus-demo-69c67dc5dd-nspxp" podUID="87df75eb-ded5-4e3d-8110-642fb6ed719f"
Jun 18 03:18:20 minikube kubelet[2486]: E0618 03:18:20.931955    2486 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"quarkus-demo\" with ImagePullBackOff: \"Back-off pulling image \\\"yourusername/quarkus-demo\\\": ErrImagePull: Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/quarkus-demo-69c67dc5dd-nspxp" podUID="87df75eb-ded5-4e3d-8110-642fb6ed719f"
Jun 18 03:18:32 minikube kubelet[2486]: E0618 03:18:32.925997    2486 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"quarkus-demo\" with ImagePullBackOff: \"Back-off pulling image \\\"yourusername/quarkus-demo\\\": ErrImagePull: Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/quarkus-demo-69c67dc5dd-nspxp" podUID="87df75eb-ded5-4e3d-8110-642fb6ed719f"
Jun 18 03:18:45 minikube kubelet[2486]: E0618 03:18:45.923930    2486 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"quarkus-demo\" with ImagePullBackOff: \"Back-off pulling image \\\"yourusername/quarkus-demo\\\": ErrImagePull: Error response from daemon: pull access denied for yourusername/quarkus-demo, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/quarkus-demo-69c67dc5dd-nspxp" podUID="87df75eb-ded5-4e3d-8110-642fb6ed719f"


==> storage-provisioner [93633b470d51] <==
W0618 03:18:00.957320       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:00.963256       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:02.970039       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:02.976324       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:04.990490       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:04.997741       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:07.000418       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:07.003641       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:09.010481       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:09.016067       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:11.022345       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:11.027500       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:13.035003       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:13.040805       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:15.052433       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:15.059739       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:17.068328       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:17.076588       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:19.082942       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:19.088832       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:21.098608       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:21.103970       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:23.116343       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:23.124153       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:25.141892       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:25.149405       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:27.155227       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:27.164787       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:29.172046       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:29.177870       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:31.186231       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:31.192615       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:33.198193       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:33.203634       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:35.210369       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:35.216510       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:37.221312       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:37.226641       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:39.231908       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:39.236732       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:41.244131       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:41.249901       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:43.256578       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:43.261713       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:45.269384       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:45.275549       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:47.283374       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:47.290778       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:49.300405       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:49.305428       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:51.312813       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:51.320614       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:53.325849       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:53.330570       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:55.337260       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:55.343096       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:57.352074       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:57.357145       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:59.364159       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 03:18:59.371159       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

